{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import MaxPooling2D, Flatten, Conv2D\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from helper import *\n",
    "from keras import regularizers\n",
    "from time import process_time\n",
    "from shutil import copyfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "pd.options.display.max_columns = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_PATH = \"\"\n",
    "DATA_PATH = \"data/\"\n",
    "MODEL_PATH = \"model/\"\n",
    "RESULTS_PATH = \"results/\"\n",
    "ALL_DATA_FILE = \"../../../extra_small_all_object_data_in_dictionary_format.pkl\"\n",
    "NORMALIZED_IMAGE_DATA_FILE = \"../../../extra_small_normalized_image_object_data_in_numpy_format.pkl\"\n",
    "MODEL_LOGGING_FILE = \"model_results.csv\"\n",
    "all_data = pickle.load(open(DATA_PATH + ALL_DATA_FILE, \"rb\"))\n",
    "all_images_normalized = pickle.load(open(DATA_PATH + NORMALIZED_IMAGE_DATA_FILE, \"rb\"))\n",
    "(X_train, X_train_normal, Y_train, file_path_train, observation_number_train), (X_test, X_test_normal, Y_test, file_path_test, observation_number_test) = split_space_data(all_images_normalized, all_data[\"images\"],all_data[\"targets\"], all_data[\"file_paths\"], all_data[\"observation_numbers\"], 0.1)\n",
    "(X_train, X_train_normal, Y_train, file_path_train, observation_number_train), (X_valid, X_valid_normal, Y_valid, file_path_valid, observation_number_valid) = split_space_data(X_train,X_train_normal,Y_train,file_path_train,observation_number_train,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X, Y, params):\n",
    "    \n",
    "    # Figure out the data shape\n",
    "    input_shape = (X.shape[1], X.shape[2], X.shape[3])\n",
    "    \n",
    "    # Define the model object to append layers to\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add first layer\n",
    "    model.add(Conv2D(\n",
    "        filters=params[\"NUMBER_OF_FILTERS_1\"],\n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        padding='same',\n",
    "        data_format='channels_last',\n",
    "        input_shape=input_shape\n",
    "    ))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(\n",
    "        filters=params[\"NUMBER_OF_FILTERS_1\"],\n",
    "        kernel_size=(3,3),\n",
    "        strides=(2,2),\n",
    "        padding='same',\n",
    "        data_format='channels_last',\n",
    "        input_shape=input_shape\n",
    "    ))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # Second layer\n",
    "    model.add(Conv2D(\n",
    "        filters=params[\"NUMBER_OF_FILTERS_2\"],\n",
    "        strides=(1,1),\n",
    "        kernel_size=(3,3),\n",
    "        padding='same',\n",
    "        data_format='channels_last',\n",
    "    ))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(\n",
    "        filters=params[\"NUMBER_OF_FILTERS_2\"],\n",
    "        strides=(2,2),\n",
    "        kernel_size=(3,3),\n",
    "        padding='same',\n",
    "        data_format='channels_last',\n",
    "    ))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # Third layer\n",
    "    model.add(Conv2D(\n",
    "        filters=params[\"NUMBER_OF_FILTERS_3\"],\n",
    "        strides=(1,1),\n",
    "        kernel_size=(3,3),\n",
    "        padding='same',\n",
    "        data_format='channels_last',\n",
    "    ))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(\n",
    "        filters=params[\"NUMBER_OF_FILTERS_3\"],\n",
    "        strides=(2,2),\n",
    "        kernel_size=(3,3),\n",
    "        padding='same',\n",
    "        data_format='channels_last',\n",
    "    ))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # Fourth layer\n",
    "    model.add(Conv2D(\n",
    "        filters=params[\"NUMBER_OF_FILTERS_4\"],\n",
    "        strides=(1,1),\n",
    "        kernel_size=(3,3),\n",
    "        padding='same',\n",
    "        data_format='channels_last',\n",
    "    ))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(\n",
    "        filters=params[\"NUMBER_OF_FILTERS_4\"],\n",
    "        strides=(2,2),\n",
    "        kernel_size=(3,3),\n",
    "        padding='same',\n",
    "        data_format='channels_last',\n",
    "    ))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # Fifth layer\n",
    "    model.add(Conv2D(\n",
    "        filters=params[\"NUMBER_OF_FILTERS_4\"],\n",
    "        strides=(1,1),\n",
    "        kernel_size=(3,3),\n",
    "        padding='same',\n",
    "        data_format='channels_last',\n",
    "    ))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # Output layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(params[\"DROPOUT_PERCENT\"]))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_params = {\n",
    "    \"INITIALS\": \"cc\",\n",
    "    \"MODEL_DESCRIPTION\": \"My first public model!\",\n",
    "    \"VERSION\": \"1\"\n",
    "}\n",
    "\n",
    "model_params = {\n",
    "    \"LEARNING_RATE\": 0.00014148226882681195,\n",
    "    \"BATCH_SIZE\": 368,\n",
    "    \"DROPOUT_PERCENT\": 0.4488113054975806,\n",
    "    \"NUMBER_OF_FILTERS_1\": 25,\n",
    "    \"NUMBER_OF_FILTERS_2\": 63,\n",
    "    \"NUMBER_OF_FILTERS_3\": 119,\n",
    "    \"NUMBER_OF_FILTERS_4\": 210,    \n",
    "    \"NUMBER_OF_EPOCHS\": 40,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_AMOUNT = 1\n",
    "\n",
    "for current_model_number in range(MODEL_AMOUNT):\n",
    "    \n",
    "    # Indicate and log model start\n",
    "    print(\"START MODEL SEARCH (model {} of {})\".format(current_model_number, MODEL_AMOUNT))\n",
    "    start = process_time()\n",
    "    \n",
    "    # Randomize specific parameters if we are doing a search\n",
    "    # Feel free to add or change the current parameters\n",
    "    if MODEL_AMOUNT > 1:\n",
    "        params[\"LEARNING_RATE\"] = 10 ** np.random.uniform(-4, -2)\n",
    "        params[\"BATCH_SIZE\"] = 16 * np.random.randint(1, 96)\n",
    "        params[\"DROPOUT_PERCENT\"] = np.random.uniform(0.0, 0.6)\n",
    "        params[\"NUMBER_OF_FILTERS_1\"] = np.random.randint(4, 32)\n",
    "        params[\"NUMBER_OF_FILTERS_2\"] = np.random.randint(16, 64)\n",
    "        params[\"NUMBER_OF_FILTERS_3\"] = np.random.randint(32, 128)\n",
    "        params[\"NUMBER_OF_FILTERS_4\"] = np.random.randint(64, 256) \n",
    "        \n",
    "    # Build the model and catch if the model acrhitectur is not valid\n",
    "    try:\n",
    "        model = build_model(X_train, Y_train, model_params)\n",
    "    except Exception as e:\n",
    "        print(\"That didn't work!\")\n",
    "        print(e)\n",
    "        continue\n",
    "        \n",
    "    # Create the specific model name\n",
    "    model_name = user_params[\"INITIALS\"] + \"_convolutional_\" + str(user_params[\"VERSION\"]) + str(current_model_number)\n",
    "    user_params[\"VERSION\"] = user_params[\"VERSION\"] + str(1)\n",
    "    \n",
    "    # Define an optimizer for the model\n",
    "    adam_optimizer = Adam(\n",
    "        learning_rate=model_params[\"LEARNING_RATE\"], \n",
    "        beta_1=0.9, \n",
    "        beta_2=0.999, \n",
    "        epsilon=None, \n",
    "        decay=0.0\n",
    "    )\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\", \n",
    "        optimizer=adam_optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Figure out where to save the model checkpoints\n",
    "    checkpoint_file = MODEL_PATH + \"mdl.hdf5\"\n",
    "    checkpointer = ModelCheckpoint(filepath=checkpoint_file, verbose=2, save_best_only=True)\n",
    "    \n",
    "    # Create an early stopping callback\n",
    "    early_stopping_callback = EarlyStopping(patience=5, min_delta=0.0005, verbose=2)\n",
    "    \n",
    "    # Actually train the model\n",
    "    print(model_params)\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        batch_size=model_params[\"BATCH_SIZE\"],\n",
    "        epochs=model_params[\"NUMBER_OF_EPOCHS\"],\n",
    "        verbose=1,\n",
    "        validation_data=(X_valid, Y_valid),\n",
    "        callbacks=[checkpointer, early_stopping_callback]\n",
    "    )\n",
    "    \n",
    "    # Reload the best model\n",
    "    model = load_model(checkpoint_file)\n",
    "\n",
    "    # Get final predictions for the model and write to a file\n",
    "    predictions = model.predict(X_test).flatten()\n",
    "    model_metrics = get_metrics(predictions, Y_test)\n",
    "    create_result_csv(user_params, model_params, model_metrics, file_name=RESULTS_PATH + MODEL_LOGGING_FILE)\n",
    "\n",
    "    # Plot the model history\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Training History')\n",
    "    plt.ylabel('Binary Cross Entropy Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xlim([0, len(history.history['loss'])])\n",
    "    plt.legend(['Training set', 'Validation set'], loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    # Reset plot to clean up extra lines\n",
    "    plt.clf()\n",
    "\n",
    "    # Get some indication of process length\n",
    "    final = process_time()\n",
    "    print('FINISHED MODEL SEARCH. {} SECONDS.'.format(str(final-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
